{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "في هذا النشاط سنصنف البيانات بناء على أسلوب المؤلفين وبالاعتماد على الكتب المنشورة. وتلكم دراسة الحالة نفسها التي قدمت في المبحث."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from text_analytics import TextAnalytics\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ai = TextAnalytics()\n",
    "ai.data_dir = os.path.join(\"..\", \"data\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "لنحمّل الآن الكتب التي نحتاجها لكل مدينة. وبحثنا هو في المؤلفين الذين ولدوا بين عامي 1850-1900، ولكل منهم هنا عدد من الكتب. وفي ذلك ضمان أننا نتعلم *التعميم* حتى لا نتنبأ فقط بالكتب الفردية. وكل عينة لـ 5 آلاف جزء من كتاب واحد."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Author                Title  \\\n",
      "0      abbott_j  alexander_the_great   \n",
      "1      abbott_j  alexander_the_great   \n",
      "2      abbott_j  alexander_the_great   \n",
      "3      abbott_j  alexander_the_great   \n",
      "4      abbott_j  alexander_the_great   \n",
      "...         ...                  ...   \n",
      "15994    wood_h       victor_serenus   \n",
      "15995    wood_h       victor_serenus   \n",
      "15996    wood_h       victor_serenus   \n",
      "15997    wood_h       victor_serenus   \n",
      "15998    wood_h       victor_serenus   \n",
      "\n",
      "                                                    Text  \n",
      "0      note project gutenberg also has an html versio...  \n",
      "1      it will be recollected to epirus where her fri...  \n",
      "2      it would be best to endeavor to effect a landi...  \n",
      "3      transport his army across the straits the army...  \n",
      "4      that the true greatness of the soul of alexand...  \n",
      "...                                                  ...  \n",
      "15994  since i have been with amabel it hath waxed st...  \n",
      "15995  his face uttered a loud cry and shrank back af...  \n",
      "15996  the taurus mountains made the afternoon balmy ...  \n",
      "15997  me to a place not very far distant where all m...  \n",
      "15998  never knew these things before now thou dost r...  \n",
      "\n",
      "[15999 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "file = \"stylistics.authorship_1850.gz\"\n",
    "file = os.path.join(ai.data_dir, file)\n",
    "df = pd.read_csv(file)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "وسنستعمل دالة التتابعات اللفظية word n-grams. وتلك التتابعات اللفظية قد حددت مسبقا، فلا نحتاج إلى تشغيل نموذج قبل استعمالهم. فلنلق نظرة الآن على عدد المؤلفين لدينا."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abbott_j 927\n",
      "altsheler_j 610\n",
      "bennett_a 573\n",
      "bindloss_h 815\n",
      "chambers_r 728\n",
      "collingwood_h 659\n",
      "collins_w 858\n",
      "crawford_f 912\n",
      "doyle_a 694\n",
      "galsworthy_j 337\n",
      "gissing_g 528\n",
      "haggard_h 956\n",
      "hume_f 975\n",
      "london_j 590\n",
      "meade_l 701\n",
      "oppenheim_e 848\n",
      "parker_g 429\n",
      "quiller-couch_a 514\n",
      "stratemeyer_e 792\n",
      "ward_h 671\n",
      "warner_c 300\n",
      "wells_c 515\n",
      "weyman_s 511\n",
      "wood_h 556\n"
     ]
    }
   ],
   "source": [
    "freq = ai.print_labels(df, \"Author\")\n",
    "\n",
    "for author in freq:\n",
    "    print(author, freq[author])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "وتلك عينات كثيرة لكل مؤلف. ولذلك سيصعب علينا إجراء تعميمات تعرّفنا بمن كتب ما نتج من استعمال التتابعات اللفظية فقط نحو: \"there was\" أو \"in the\". إذن حتى الآن عندنا (1) بياناتنا من مشروع قوتنبيرق، و(2) متجه أسلوب هذه البيانات (دالة التتابعات اللفظية). وسنصنف البيانات حسب مؤلفها. وأدناه الكود البرمجي الأساسي، وفيه فقط استدعاء لحزمتنا *text_analytics*. وتلك الحزمة تقسم البيانات إلى بيانات تدريب وبيانات اختبار، ثم تدرب المصنف وتقيمه. وتلك عينات كثيرة لكل مؤلف. ولذلك سيصعب علينا إجراء تعميمات تعرّفنا بمن كتب ما نتج من استعمال التتابعات اللفظية فقط نحو: \"there was\" أو \"in the\". وقد وجهنا الحزمة لاستعمال \"الموئلف  \"Author صنفا أساسيا مع الخصائص الأسلوبية.\n",
    "\n",
    "**طوّل بالك**\n",
    "\n",
    "هذا كل ما في الأمر! ننظر في دقة المصنف.\n",
    "وسيتغير ذلك عما في المحاضرة قليلا لأننا نستعمل فصلا عشوائيا لبيانات التدريب والاختبار. وهذا يعني أن المصنف ينظر في بيانات مختلفة كل مرة. وإذا ما أردت أمثلة إضافية متقدمة على حل إشكالية تصنيف نصوص المؤلفين، انظر في دالة *text_analytics.shallow_classification.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       abbott_j       0.99      0.99      0.99        85\n",
      "    altsheler_j       1.00      1.00      1.00        60\n",
      "      bennett_a       1.00      0.98      0.99        53\n",
      "     bindloss_h       1.00      1.00      1.00        82\n",
      "     chambers_r       1.00      1.00      1.00        68\n",
      "  collingwood_h       1.00      0.99      0.99        70\n",
      "      collins_w       1.00      1.00      1.00        86\n",
      "     crawford_f       1.00      0.98      0.99       100\n",
      "        doyle_a       1.00      0.99      0.99        76\n",
      "   galsworthy_j       1.00      0.97      0.98        33\n",
      "      gissing_g       1.00      1.00      1.00        45\n",
      "      haggard_h       0.99      1.00      0.99        99\n",
      "         hume_f       0.98      1.00      0.99        94\n",
      "       london_j       0.97      1.00      0.99        70\n",
      "        meade_l       1.00      1.00      1.00        65\n",
      "    oppenheim_e       1.00      1.00      1.00        88\n",
      "       parker_g       0.98      1.00      0.99        44\n",
      "quiller-couch_a       0.96      1.00      0.98        55\n",
      "  stratemeyer_e       1.00      0.97      0.99        75\n",
      "         ward_h       0.96      0.98      0.97        53\n",
      "       warner_c       1.00      0.97      0.99        34\n",
      "        wells_c       1.00      1.00      1.00        50\n",
      "       weyman_s       0.98      1.00      0.99        45\n",
      "         wood_h       1.00      0.99      0.99        70\n",
      "\n",
      "       accuracy                           0.99      1600\n",
      "      macro avg       0.99      0.99      0.99      1600\n",
      "   weighted avg       0.99      0.99      0.99      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = ai.shallow_classification(df, labels = \"Author\", features = \"style\", classifier = \"lm\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**طوّل بالك**\n",
    "\n",
    "هذا كل ما في الأمر! ننظر في دقة المصنف.\n",
    "وسيتغير ذلك عما في المحاضرة قليلا لأننا نستعمل فصلا عشوائيا لبيانات التدريب والاختبار. وهذا يعني أن المصنف ينظر في بيانات مختلفة كل مرة. وإذا ما أردت أمثلة إضافية متقدمة على حل إشكالية تصنيف نصوص المؤلفين، انظر في دالة *text_analytics.shallow_classification.\n"
   ]
  }
 ],
 "metadata": {
  "direction": "rtl",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
