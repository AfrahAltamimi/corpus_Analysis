{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اليوم سنستعمل المتجهات الموقعية للتنبؤ بالقسم الكلامي الذي تنتمي له الكلمة. و *المتجهه الموقعي* يرمّز السياق في الجملة، فيما ترمّز المتجهات الأخرى التي قد استعملناها خصائص لغوية محددة بغض النظر عن مواقعها. ونبدأ كالمعتاد بتحميل متطلبات العمل."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from text_analytics import TextAnalytics\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ai = TextAnalytics()\n",
    "ai.data_dir = os.path.join(\"..\", \"data\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "وهذه المرة سنعمل على مدونات من مشروع التبعيات العالمي الشجري Universal Dependencies project project (universaldependencies.org). وقد عدلنا على المدونات الأصلية، ولكن التنسيق بقي مشابها جدا. فكل كلمة عبارة عن صف، ويمثل شكل الكلمة عمودا واحدا، ووسم القسم الكلامي عمودا ثانيا. ولدينا أيضا معلومات تتعلق بالجملة. وهذا يدفعنا للتأكد من أننا لم نرمز سلاسل الكلمات التي تتجاوز حدود الجملة."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Source  Sentence_ID Word_ID    Word    POS\n",
      "0       en_ewt            1       1    from    ADP\n",
      "1       en_ewt            1       2     the    DET\n",
      "2       en_ewt            1       3      ap  PROPN\n",
      "3       en_ewt            1       4   comes   VERB\n",
      "4       en_ewt            1       5    this    DET\n",
      "...        ...          ...     ...     ...    ...\n",
      "559457  en_pud        32356      21       a    DET\n",
      "559458  en_pud        32356      22  friend   NOUN\n",
      "559459  en_pud        32356      23      of    ADP\n",
      "559460  en_pud        32356      24   peace   NOUN\n",
      "559461  en_pud        32356      25       .  PUNCT\n",
      "\n",
      "[559462 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "file = os.path.join(ai.data_dir, \"syntax.pos_english.gz\")\n",
    "df = pd.read_csv(file)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ولننشئ الآن إطارا للبيانات وتجربة جملة واحدة فقط."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Source  Sentence_ID Word_ID         Word    POS\n",
      "7   en_ewt            2       1    president  PROPN\n",
      "8   en_ewt            2       2         bush  PROPN\n",
      "9   en_ewt            2       3           on    ADP\n",
      "10  en_ewt            2       4      tuesday  PROPN\n",
      "11  en_ewt            2       5    nominated   VERB\n",
      "12  en_ewt            2       6          two    NUM\n",
      "13  en_ewt            2       7  individuals   NOUN\n",
      "14  en_ewt            2       8           to   PART\n",
      "15  en_ewt            2       9      replace   VERB\n",
      "16  en_ewt            2      10     retiring   VERB\n",
      "17  en_ewt            2      11      jurists   NOUN\n",
      "18  en_ewt            2      12           on    ADP\n",
      "19  en_ewt            2      13      federal    ADJ\n",
      "20  en_ewt            2      14       courts   NOUN\n",
      "21  en_ewt            2      15           in    ADP\n",
      "22  en_ewt            2      16          the    DET\n",
      "23  en_ewt            2      17   washington  PROPN\n",
      "24  en_ewt            2      18         area   NOUN\n",
      "25  en_ewt            2      19            .  PUNCT\n"
     ]
    }
   ],
   "source": [
    "test_df = df.loc[df.loc[:,\"Sentence_ID\"]==2]\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "وهذا الكود أدناه سيكرر على كل كلمة في الجملة ويولد متجها موقعيا لتلك الكلمة المحددة متضمنا ذلك المتجه كلمتين سابقتين وكلمتين تاليتين. ثم نطبع المتجه ونحفظه."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', '#', 'president', 'bush', 'on']\n",
      "['#', 'president', 'bush', 'on', 'tuesday']\n",
      "['president', 'bush', 'on', 'tuesday', 'nominated']\n",
      "['bush', 'on', 'tuesday', 'nominated', 'two']\n",
      "['on', 'tuesday', 'nominated', 'two', 'individuals']\n",
      "['tuesday', 'nominated', 'two', 'individuals', 'to']\n",
      "['nominated', 'two', 'individuals', 'to', 'replace']\n",
      "['two', 'individuals', 'to', 'replace', 'retiring']\n",
      "['individuals', 'to', 'replace', 'retiring', 'jurists']\n",
      "['to', 'replace', 'retiring', 'jurists', 'on']\n",
      "['replace', 'retiring', 'jurists', 'on', 'federal']\n",
      "['retiring', 'jurists', 'on', 'federal', 'courts']\n",
      "['jurists', 'on', 'federal', 'courts', 'in']\n",
      "['on', 'federal', 'courts', 'in', 'the']\n",
      "['federal', 'courts', 'in', 'the', 'washington']\n",
      "['courts', 'in', 'the', 'washington', 'area']\n",
      "['in', 'the', 'washington', 'area', '.']\n",
      "['the', 'washington', 'area', '.', '#']\n",
      "['washington', 'area', '.', '#', '#']\n"
     ]
    }
   ],
   "source": [
    "x_vectors = []\n",
    "y_vector = []\n",
    "\n",
    "#A list of words and a list of ground-truth labels\n",
    "words = test_df.loc[:,\"Word\"].values\n",
    "tags = test_df.loc[:,\"POS\"].values\n",
    "            \n",
    "#Create a positional vector for each word in the sentence\n",
    "for i in range(len(words)):\n",
    "    y_vector.append(tags[i])\n",
    "    vector = []\n",
    "                \n",
    "    #Find the correct context window, filling in slots at the edges\n",
    "    for j in [-2, -1, 0, 1, 2]:\n",
    "        if i+j < 0 or i+j > len(words)-1:\n",
    "            vector.append(\"#\")\n",
    "        else:\n",
    "            vector.append(words[i+j])\n",
    "                        \n",
    "    #Save the positional vector for this word\n",
    "    print(vector)\n",
    "    x_vectors.append(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "إن تلك السلاسل من الكلمات هي مجرد مصفوفات من السلاسل الحرفية. فلا بد من تحويل ذلك إلى خط الترميز الأحادي one-hot encoding حيث يكون لكل موضع فيه عمود فريد في المتجه. وهنا نحول تلك السلاسل الحرفية إلى خطوط الترميزات الأحادية، ثم نطبع المصفوفات كاملة لنرى كيف تبدو. إن كل صف (كل مصفوفة جديدة) يمثل كلمة واحدة فقط، والسياق المحيط بها."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#With all sentences finished, conert into numpy array\n",
    "x_vectors = np.array(x_vectors)\n",
    "y_vector = np.array(y_vector)\n",
    "\n",
    "#Convert into a one-hot encoding\n",
    "encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "encoder.fit(x_vectors)\n",
    "        \n",
    "#Return the x and y vectors\n",
    "x_vectors = encoder.transform(x_vectors)\n",
    "print(x_vectors.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ما مدى نجاعة هذه المنهجية في التوسيم الآلي لأقسام الكلام؟\n",
    "هيا نستكشف!\n",
    "\n",
    "\n",
    "سيستعمل السطر أدناه حزمة *text_analytics* لتوليد خط ترميز أحادي موقعي لما يقارب نصف مليون كلمة، والتنبؤ بوسوم أقسام الكلام، باستعمال الانحدار اللوجيستي.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.89      0.85      0.87      3768\n",
      "         ADP       0.93      0.97      0.95      5299\n",
      "         ADV       0.90      0.81      0.85      2613\n",
      "         AUX       0.95      0.98      0.97      3018\n",
      "       CCONJ       0.99      0.99      0.99      1820\n",
      "         DET       0.98      0.97      0.98      4637\n",
      "        INTJ       0.98      0.76      0.86       156\n",
      "        NOUN       0.86      0.95      0.90      9999\n",
      "         NUM       0.95      0.84      0.89      1002\n",
      "        PART       0.95      0.97      0.96      1339\n",
      "        PRON       0.96      0.97      0.96      4561\n",
      "       PROPN       0.85      0.74      0.79      3175\n",
      "       PUNCT       0.99      1.00      1.00      6760\n",
      "       SCONJ       0.84      0.75      0.79      1045\n",
      "         SYM       0.95      0.68      0.79       116\n",
      "        VERB       0.93      0.91      0.92      5992\n",
      "           X       0.88      0.36      0.51       145\n",
      "           _       0.98      0.98      0.98       502\n",
      "\n",
      "    accuracy                           0.93     55947\n",
      "   macro avg       0.93      0.86      0.89     55947\n",
      "weighted avg       0.93      0.93      0.93     55947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = ai.pos_tagger(df, classifier=\"lm\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ثمة مناهج دائما قد تستعمل لتحسين أداء عمليات التوسيم. وهذا النشاط يبين منهجية استعمال نافذة سياق بسيطة من كلمتين لتصنيف الخصائص النحوية للكلمات."
   ]
  }
 ],
 "metadata": {
  "direction": "rtl",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
