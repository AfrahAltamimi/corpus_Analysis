{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "سنستعمل اليوم منهجية مختلفة في التشابه، وهي إيجاد الارتباط بين الكلمات. وهي نفس الفكرة التي تبنيناها عند استهداف الوقوف على العبارات (مقياس المعلومات المتبادلة النقطيPointwise Mutual Information). غير أننا هنا سنستعمل مقياسا محدد الاتجاه لتقديم نتائج أكثر دقة من الناحية التركيبية.\n",
    "\n",
    "فلنبدأ بتجهيز بياناتنا.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from text_analytics import TextAnalytics\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ai = TextAnalytics()\n",
    "ai.data_dir = os.path.join(\"..\", \"data\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "سننطلق بالنظر في بياناتنا من مشروع قوتنبيرق. وإذ لا نحتاج لقاعدة البيانات كاملة، سنرفع جزءا منها وحسب. وذلك لأننا سنقيم دالة مقياس الارتباط. وسنرفع لاحقا كامل النتائج التي قد عولجت."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Author                Title  \\\n",
      "0       abbott_j  alexander_the_great   \n",
      "1       abbott_j  alexander_the_great   \n",
      "2       abbott_j  alexander_the_great   \n",
      "3       abbott_j  alexander_the_great   \n",
      "4       abbott_j  alexander_the_great   \n",
      "..           ...                  ...   \n",
      "995  altsheler_j        the_candidate   \n",
      "996  altsheler_j        the_candidate   \n",
      "997  altsheler_j        the_candidate   \n",
      "998  altsheler_j        the_candidate   \n",
      "999  altsheler_j        the_candidate   \n",
      "\n",
      "                                                  Text  \n",
      "0    note project gutenberg also has an html versio...  \n",
      "1    it will be recollected to epirus where her fri...  \n",
      "2    it would be best to endeavor to effect a landi...  \n",
      "3    transport his army across the straits the army...  \n",
      "4    that the true greatness of the soul of alexand...  \n",
      "..                                                 ...  \n",
      "995  comrades yet churchill was not wholly pleased ...  \n",
      "996  idea and he immediately hunted up the cousin a...  \n",
      "997  station and presently she met king plummer com...  \n",
      "998  be in to breakfast you have diagnosed his chie...  \n",
      "999  to mitigate things stepped forward i beg your ...  \n",
      "\n",
      "[1000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "file = os.path.join(ai.data_dir, \"stylistics.authorship_1850.gz\")\n",
    "df = pd.read_csv(file, nrows = 1000)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "والآن سنستعد للوقوف على أكثر أزواج الكلمات ارتباطا. ويتيح خيار *save_phraser* استعمال ذلك أساسا للوقوف على العبارات أيضا. ولكنا هنا سنراجع النتائج فقط."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTime: 709.6022391319275 Full: 989787  Reduced: 392987 with 4997883 words.\n",
      "\n",
      "\tTOTAL TIME: 709.6644697189331\n",
      "\tTOTAL NGRAMS: 392987\n",
      "\tTOTAL WORDS: 4997883\n",
      "\tAfter pruning:\n",
      "\tTOTAL NGRAMS: 382815\n",
      "\n",
      "\tCalculating association for 382815 pairs.\n",
      "\tProcessed 382815 items in 5.496898174285889\n",
      "         Word1    Word2       Max        LR        RL  Freq\n",
      "347524      de  peyster  0.998835  0.998835  0.063444   126\n",
      "57412    madam   rachel  0.998102  0.998102  0.414062    53\n",
      "241275  madame   campan  0.997126  0.997126  0.039785    37\n",
      "290489    loch    leven  0.997059  0.997059  0.447368    34\n",
      "24262    rhode   island  0.997006  0.031223  0.997006    36\n",
      "...        ...      ...       ...       ...       ...   ...\n",
      "59263      and      and -0.036657 -0.036657 -0.036657   127\n",
      "4039       the       of -0.037537 -0.068729 -0.037537  1891\n",
      "16027      the      and -0.037744 -0.075858 -0.037744   433\n",
      "36766       of       of -0.041107 -0.041107 -0.041107    47\n",
      "3216       the      the -0.078698 -0.078698 -0.078698  1047\n",
      "\n",
      "[348964 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "association_df = ai.get_association(df, min_count = 1, save_phraser = True)\n",
    "print(association_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "وهذا مقياس محدد الاتجاه، لذا لدينا رقمان أساسيان: من اليسار لليمين LR – من اليمين لليسار RL. ولدينا أيضا عمود *max* الذي يتيح لنا الاتجاه الأفضل لكل زوج من الكلمات. ونرى أخيرا التكرار. وهذا المقياس لا يتأثر بالتكرار مثل تأثر PMI، غير أن ثمة انحياز قائم نحو الأزواج النادرة من الكلمات التي ستظهر بدرجة ارتباط كبيرة.\n",
    "\n",
    "والآن لنرفع أزواج الكلمات المدربة تدريبا كاملا من قاعدة بيانات مشروع قوتنبيرق وقاعدة بيانات الويب\\التغريدات.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Word1      Word2       Max        LR        RL    Freq\n",
      "0          le    gardeur  0.999871  0.999871  0.022920    1344\n",
      "1           m     kinlay  0.999788  0.999788  0.010918     638\n",
      "2         des  esseintes  0.999738  0.999738  0.011821     442\n",
      "3         des    hermies  0.999672  0.999672  0.009146     342\n",
      "4         don     aníbal  0.999594  0.999594  0.002330     464\n",
      "...       ...        ...       ...       ...       ...     ...\n",
      "3987819   the        and -0.032834 -0.061111 -0.032834  182982\n",
      "3987820   the         of -0.032943 -0.059793 -0.032943  235270\n",
      "3987821   and        and -0.033293 -0.033293 -0.033293   44364\n",
      "3987822    of         of -0.035106 -0.035106 -0.035106   15540\n",
      "3987823   the        the -0.067346 -0.067346 -0.067346   87214\n",
      "\n",
      "[3987824 rows x 6 columns]\n",
      "           Word1        Word2       Max        LR        RL   Freq\n",
      "0        gustado           un  0.999816  0.014754  0.999816    870\n",
      "1         indigo       cafind  0.999745  0.999745  0.129993    397\n",
      "2          votes  dullaverage  0.999718  0.999718  0.008584    456\n",
      "3        cookies       okread  0.999666  0.999666  0.021476    316\n",
      "4         rodong       sinmun  0.999636  0.999636  0.975177    275\n",
      "...          ...          ...       ...       ...       ...    ...\n",
      "4547271      the           of -0.021413 -0.041590 -0.021413  14004\n",
      "4547272      the          and -0.021730 -0.041168 -0.021730  22209\n",
      "4547273       to           to -0.023303 -0.023303 -0.023303  77309\n",
      "4547274      the           to -0.026867 -0.041895 -0.026867  15820\n",
      "4547275      the          the -0.042298 -0.042298 -0.042298  33977\n",
      "\n",
      "[4547276 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "file = os.path.join(ai.data_dir, \"stylistics.gutenberg_all.gz.association.gz\")\n",
    "pg_df = pd.read_csv(file)\n",
    "print(pg_df)\n",
    "\n",
    "file = os.path.join(ai.data_dir, \"sociolinguistics.english_all.gz.association.gz\")\n",
    "tw_df = pd.read_csv(file)\n",
    "print(tw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "لنأخذ بعض العينات من العبارات المرتبطة جدا باتجاه (اليسار لليمين) من مجموعتي البيانات."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Word1      Word2       Max        LR            RL  Freq\n",
      "755         s        eep  0.990731  0.990731  1.146111e-05    12\n",
      "1369      the    hallers  0.932914  0.932914  5.370297e-07    36\n",
      "1420      the    mayhews  0.931535  0.931535  3.579856e-07    24\n",
      "1522      the    towrope  0.929473  0.929473  2.386229e-07    16\n",
      "536       las     mismas  0.992902  0.992902  2.182045e-03    14\n",
      "...       ...        ...       ...       ...           ...   ...\n",
      "77    captain   battleax  0.998232  0.998232  1.966822e-04    70\n",
      "1768       of       beor  0.920211  0.920211  1.171863e-06    42\n",
      "1613       de  barrameda  0.928121  0.928121  5.542029e-05    26\n",
      "914        de     randol  0.970625  0.970625  1.001850e-03   470\n",
      "810        at       salò  0.985562  0.985562  1.865128e-06    12\n",
      "\n",
      "[1607 rows x 6 columns]\n",
      "         Word1             Word2       Max        LR        RL  Freq\n",
      "748     batang         berjuntai  0.994152  0.994152  0.042079    17\n",
      "3022  libelium          waspmote  0.965517  0.965517  0.736842    28\n",
      "6065        to           abancay  0.909278  0.909278  0.000001    29\n",
      "3392      homo        economicus  0.958331  0.958331  0.013514    23\n",
      "752        mol            ecules  0.994151  0.994151  0.014395    17\n",
      "...        ...               ...       ...       ...       ...   ...\n",
      "1392       van              nuys  0.991263  0.991263  0.003279   114\n",
      "6030        de           broglio  0.910970  0.910970  0.000347    41\n",
      "4346  waptrick           wapking  0.940000  0.940000  0.431193    47\n",
      "2076    titles  descriptionstore  0.983647  0.983647  0.033278   542\n",
      "2192    cancel          replyyou  0.981582  0.981582  0.008855   267\n",
      "\n",
      "[3730 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "pg_lr_df = pg_df.loc[pg_df[\"LR\"] > 0.90].sample(frac = 1)\n",
    "print(pg_lr_df)\n",
    "\n",
    "tw_lr_df = tw_df.loc[tw_df[\"LR\"] > 0.90].sample(frac = 1)\n",
    "print(tw_lr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "والآن يمكننا تكرار نفس الكود البرمجي، ولكن هذه المرة للوقوف على العبارات باتجاه (اليمين لليسار).وهذا كل شيء! \n",
    "\n",
    "في هذا التدريب العملي رأينا كيف يمكن الوقوف على الكلمات الأكثر ارتباطا أو استرجاعها مع مراعاة اتجاه الارتباط بين الكلمتين. ولأننا خلطنا مجموعتي البيانات، يمكنك تكرار الخلايا لإظهار المزيد من الأمثلة. إن تلك المقاييس تقف على أنماط أكثر تحديدا من مقياس المعلومات المتبادلة النقطي PMI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Word1     Word2       Max            LR        RL  Freq\n",
      "1615     edmondo        de  0.928121  5.542029e-05  0.928121    26\n",
      "102     loizeaux  brothers  0.997779  8.592990e-04  0.997779    46\n",
      "1059       zebah       and  0.959456  4.015062e-07  0.959456    14\n",
      "1069  millionths        of  0.959454  4.470817e-07  0.959454    16\n",
      "867        hetch    hetchy  0.977273  9.772727e-01  0.977273    86\n",
      "...          ...       ...       ...           ...       ...   ...\n",
      "461    redcrosse    knight  0.993751  4.070003e-04  0.993751    16\n",
      "272         kuhr         d  0.995755  2.414000e-04  0.995755    24\n",
      "1220      hoggle       end  0.946992  9.180296e-05  0.946992    36\n",
      "645      geerdes    norris  0.991731  2.808989e-03  0.991731    12\n",
      "356      forston    street  0.994863  1.183459e-04  0.994863    20\n",
      "\n",
      "[475 rows x 6 columns]\n",
      "                       Word1       Word2       Max        LR        RL  Freq\n",
      "3172                bhagavad        gita  0.963285  0.302415  0.963285   551\n",
      "5577  advertisementsubscribe          to  0.917456  0.000003  0.917456    67\n",
      "1889                 gameson  funnygames  0.986457  0.077176  0.986457   656\n",
      "3621              winterless       north  0.954352  0.000129  0.954352    21\n",
      "4282               inspectah        deck  0.941159  0.001105  0.941159    16\n",
      "...                      ...         ...       ...       ...       ...   ...\n",
      "5795               tegmental        area  0.916386  0.000047  0.916386    11\n",
      "1330                  morung     express  0.991676  0.000238  0.991676    12\n",
      "3594                caprylyl      glycol  0.954545  0.080051  0.954545    63\n",
      "3874          elizabethville          is  0.949389  0.000005  0.949389    51\n",
      "622                  functus     officio  0.994764  0.063973  0.994764    19\n",
      "\n",
      "[2875 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "pg_rl_df = pg_df.loc[pg_df[\"RL\"] > 0.90].sample(frac = 1)\n",
    "print(pg_rl_df)\n",
    "\n",
    "tw_rl_df = tw_df.loc[tw_df[\"RL\"] > 0.90].sample(frac = 1)\n",
    "print(tw_rl_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "وهذا كل شيء! \n",
    "\n",
    "في هذا التدريب العملي رأينا كيف يمكن الوقوف على الكلمات الأكثر ارتباطا أو استرجاعها مع مراعاة اتجاه الارتباط بين الكلمتين. ولأننا خلطنا مجموعتي البيانات، يمكنك تكرار الخلايا لإظهار المزيد من الأمثلة. إن تلك المقاييس تقف على أنماط أكثر تحديدا من مقياس المعلومات المتبادلة النقطي PMI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "direction": "rtl",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
