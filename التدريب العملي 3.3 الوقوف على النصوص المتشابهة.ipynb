{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "سنستعمل اليوم أحد مقاييس المسافة، وهو مقياس المسافة الإقليدية؛ وذلك لاسترجاع أو إيجاد المستندات الأكثر تشابها. والفكرة هنا هي أننا قد نمثل محتوى النص أو أسلوبه أو المشاعر فيه بمتجه رقمي. ويقاس التشابه بين هذه المتجهات كوسيلة لقياس التشابه بين النصوص التي تمثلها. وبدلا من استعمال مصنف النصوص للتمييز بين التصنيفات، سنحاول الوقوف على أكثر الأمثلة تشابها.\n",
    "\n",
    "لنبدأ بتجهيز بياناتنا."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from text_analytics import TextAnalytics\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ai = TextAnalytics()\n",
    "ai.data_dir = os.path.join(\"..\", \"data\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "وسنبدأ بالنظر في التشابه المعتمد على المؤلف باستعمال الخصائص الأسلوبية. فدعونا إذن نرفع البيانات من مشروع قوتنبيرق. ولأننا لا نحتاج لقاعدة البيانات كاملة، سنرفع جزءا منها وحسب."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Author                Title  \\\n",
      "0      abbott_j  alexander_the_great   \n",
      "1      abbott_j  alexander_the_great   \n",
      "2      abbott_j  alexander_the_great   \n",
      "3      abbott_j  alexander_the_great   \n",
      "4      abbott_j  alexander_the_great   \n",
      "...         ...                  ...   \n",
      "1995  bennett_a   the_old_wives_tale   \n",
      "1996  bennett_a   the_old_wives_tale   \n",
      "1997  bennett_a   the_old_wives_tale   \n",
      "1998  bennett_a   the_old_wives_tale   \n",
      "1999  bennett_a   the_old_wives_tale   \n",
      "\n",
      "                                                   Text  \n",
      "0     note project gutenberg also has an html versio...  \n",
      "1     it will be recollected to epirus where her fri...  \n",
      "2     it would be best to endeavor to effect a landi...  \n",
      "3     transport his army across the straits the army...  \n",
      "4     that the true greatness of the soul of alexand...  \n",
      "...                                                 ...  \n",
      "1995  the stipendiary achieved marvellously the illu...  \n",
      "1996  winter overcoat he directed the vast affair of...  \n",
      "1997  after a pause miss insull there are a few card...  \n",
      "1998  in widths lengths and prices she never erred s...  \n",
      "1999  mother would have insisted on by means of tear...  \n",
      "\n",
      "[2000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "file = os.path.join(ai.data_dir, \"stylistics.authorship_1850.gz\")\n",
    "df = pd.read_csv(file, nrows = 2000)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "والآن سنستعد للعثور على العينات الأكثر تشابها بفعل أمرين: (1) تحويل النص إلى متجهات تمثل الأسلوب، و(2) اختيار عينة عشوائية للنظر فيها. ويعني ذلك أننا نختار جزءا من النص عشوائيا ثم نبحث عن العينات الأكثر شبها به."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t338\n",
      "  (0, 1)\t273\n",
      "  (0, 2)\t256\n",
      "  (0, 3)\t125\n",
      "  (0, 4)\t88\n",
      "  (0, 5)\t121\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t115\n",
      "  (0, 8)\t48\n",
      "  (0, 9)\t102\n",
      "  (0, 10)\t25\n",
      "  (0, 11)\t118\n",
      "  (0, 13)\t62\n",
      "  (0, 14)\t28\n",
      "  (0, 15)\t32\n",
      "  (0, 16)\t33\n",
      "  (0, 17)\t18\n",
      "  (0, 18)\t8\n",
      "  (0, 19)\t16\n",
      "  (0, 20)\t16\n",
      "  (0, 21)\t35\n",
      "  (0, 22)\t34\n",
      "  (0, 23)\t4\n",
      "  (0, 24)\t27\n",
      "  (0, 25)\t13\n",
      "  :\t:\n",
      "  (1999, 209)\t2\n",
      "  (1999, 210)\t1\n",
      "  (1999, 212)\t2\n",
      "  (1999, 215)\t1\n",
      "  (1999, 218)\t2\n",
      "  (1999, 220)\t1\n",
      "  (1999, 221)\t4\n",
      "  (1999, 222)\t2\n",
      "  (1999, 224)\t3\n",
      "  (1999, 225)\t1\n",
      "  (1999, 232)\t3\n",
      "  (1999, 236)\t1\n",
      "  (1999, 237)\t1\n",
      "  (1999, 239)\t1\n",
      "  (1999, 242)\t1\n",
      "  (1999, 244)\t1\n",
      "  (1999, 250)\t3\n",
      "  (1999, 252)\t4\n",
      "  (1999, 253)\t1\n",
      "  (1999, 278)\t1\n",
      "  (1999, 281)\t2\n",
      "  (1999, 285)\t1\n",
      "  (1999, 295)\t2\n",
      "  (1999, 301)\t5\n",
      "  (1999, 303)\t1\n",
      "224\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "x, vocab_size = ai.get_features(df, \"style\")\n",
    "sample = random.randint(0,len(df))\n",
    "print(x)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "لقد أصبحنا نعرف الآن ما نبحث عنه. وها هو الكود البرمجي. ومثل كل مرة، إذا أردت التعمق أكثر، ألق نظرة على التفاصيل الفنية للحزمة *text_analytics*. إننا نستعمل الدالة *linguistic_distance* لإيجاد 5 عينات أخرى تشبه عيناتنا.\n",
    "\n",
    "المتغير *x* يمرر خصائصنا اللغوية.\n",
    "\n",
    "والمتغير *y* يمرر بياناتنا الوصفية الخاصة بأسماء المؤلفين.\n",
    "\n",
    "والمتغير *sample* الجزء النصي العشوائي الذي يجب استعماله للبحث عن المتشابهات. ويمكن أن نعيد تشغيل الكود البرمجي أدناه عددا من المرات لتوليد عينات جديدة بالقدر الذي نريد.\n",
    "\n",
    "أما المتغير *n* فيحدد عدد الأمثلة المتشابهة التي نستهدف العثور عليها.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bennett_a ['bennett_a', 'bennett_a', 'bennett_a', 'bennett_a', 'bennett_a']\n"
     ]
    }
   ],
   "source": [
    "sample = random.randint(0,len(df))\n",
    "y_sample, y_closest = ai.linguistic_distance(x = x, y = df.loc[:,\"Author\"].values, sample = sample, n = 5)\n",
    "print(y_sample, y_closest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "لنكرر ذلك مرة أخرى مع خصائص المحتوى. وهذه المرة سنبدأ برفع متجهنا TF-IDF المدرب مسبقا."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "file_phrases = os.path.join(\".\", ai.data_dir, \"sociolinguistics.english_all.gz\")\n",
    "ai.phrases = ai.deserialize(\"phrases\", file_phrases + \".phrases.json\")\n",
    "ai.tfidf_vectorizer = ai.deserialize(\"tfidf_model\", file_phrases + \".tfidf.json\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "والآن نكرر نفس الكود البرمجي باستعمال خصائص *المحتوى*. وسنلقي نظرة على تغريداتنا من مدن مختلفة."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            City                                               Text\n",
      "0     washington   you really need to go back to bar tending or ...\n",
      "1         london   jay finley christ in explains why today is co...\n",
      "2          lagos   forget if this happened truly it s definitely...\n",
      "3        toronto   yall i love this skin big thanks to for makin...\n",
      "4        nairobi   the late brilliant prof ali mazrui explains h...\n",
      "...          ...                                                ...\n",
      "1995     atlanta   according to cdc s latest levels of u s flu l...\n",
      "1996       lagos   list of the roads and bridges that buhari is ...\n",
      "1997     calgary   instead of condemning the assault of by a uni...\n",
      "1998     phoenix   also vexing how can we use the experience of ...\n",
      "1999  washington   just landed in the united kingdom heading to ...\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "file = os.path.join(ai.data_dir, \"sociolinguistics.english_cities.gz\")\n",
    "df = pd.read_csv(file, nrows = 2000)\n",
    "print(df)\n",
    "\n",
    "x, vocab_size = ai.get_features(df, \"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phoenix ['washington', 'washington', 'miami', 'washington', 'washington']\n"
     ]
    }
   ],
   "source": [
    "sample = random.randint(0,len(df))\n",
    "\n",
    "y_sample, y_closest = ai.linguistic_distance(x = x, y = df.loc[:,\"City\"].values, sample = sample, n = 5)\n",
    "print(y_sample, y_closest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "وهذا كل شيء! \n",
    "\n",
    "في هذا التدريب العملي رأينا كيف يمكن الوقوف على النصوص المتشابهة أو استرجاعها باستعمال مقياس مسافة بسيط. وقد بحثنا في أسلوب المؤلف باستعمال كتب من القرن التاسع عشر، وفي المحتوى باستعمال التغريدات. والفكرة الأساسية هنا هي أن النصوص قد تتشابه ببعضها بتلك الأساليب الثلاثة.\n",
    "\n",
    "فهل تعطي مقاييس المسافة نتائج دقيقة كالمصنفات التي استعملناها مسبقا؟ ربما لا. لكن لا بد أن تتذكر أنها أبسط كثيرا من المصنفات فهي غير موجهة؛ وهذا يعني أنه لا توجد أي بيانات تدريب.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Hotel Rating  \\\n",
      "0                 11th Avenue Hotel & Hostel    low   \n",
      "1                                3 West Club   high   \n",
      "2                                  414 Hotel   high   \n",
      "3     70 park avenue hotel - a Kimpton Hotel   high   \n",
      "4       A Victory Inn & Suites Phoenix North    low   \n",
      "...                                      ...    ...   \n",
      "4995                         The Lenox Hotel   high   \n",
      "4996                    The Listel Vancouver   high   \n",
      "4997                         The Loden Hotel   high   \n",
      "4998                            The Lombardy   high   \n",
      "4999                      The Lonsdale Hotel    low   \n",
      "\n",
      "                                                   Text  \n",
      "0     This hostel is in a very good location, close ...  \n",
      "1     We had 5 nights here and were unsure as to wha...  \n",
      "2     This is a small boutique hotel with a nice int...  \n",
      "3     I stayed at 70 Park Ave Hotel the night before...  \n",
      "4     I made a reservation. Cancelled 2 hours later ...  \n",
      "...                                                 ...  \n",
      "4995  Stayed at this hotel with a group of friends o...  \n",
      "4996  We spent a whole week at The Listel in Vancouv...  \n",
      "4997  We stayed at the Loden for the first time and ...  \n",
      "4998  My husband and I both had business in NYC, and...  \n",
      "4999  Let me start by saying, Me and my mate are two...  \n",
      "\n",
      "[5000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "file = os.path.join(ai.data_dir, \"economic.hotels_as_reviews.gz\")\n",
    "df = pd.read_csv(file, nrows = 5000)\n",
    "print(df)\n",
    "\n",
    "x, vocab_size = ai.get_features(df, \"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high ['high', 'high', 'high', 'high', 'high']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "sample = random.randint(0,len(df))\n",
    "\n",
    "y_sample, y_closest = ai.linguistic_distance(x = x, y = df.loc[:,\"Rating\"].values, sample = sample, n = 5)\n",
    "print(y_sample, y_closest)"
   ]
  }
 ],
 "metadata": {
  "direction": "rtl",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
