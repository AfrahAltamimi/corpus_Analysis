{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "في هذا النشاط سنصنف المدن بناء على المحتوى وبالعمل على عينة من التغريدات. وهي دراسة الحالة نفسها التي عملنا عليها في النشاط 2.2. ولكننا في المرة الأخيرة استعملنا مصنف انحدار لوجيستي بالاستعانة بـ *scikit-learn*. غير أن هذه المرة سنتناول نفس المشكلة ولمن باستعمال مصنف MLP (شبكة المستقبلات المتعددة الطبقات multi-layer perceptron) بالاستعانة بـ *tensorflow*.\n",
    "وكالعادة هذا النشاط يبين النسخة البسيطة الممكنة، فإذا ما أردت الاستزادة يمكنك العودة إلى الأمثلة في حزمة *text_analytics()*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from text_analytics import TextAnalytics\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ai = TextAnalytics()\n",
    "ai.data_dir = os.path.join(\"..\", \"data\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "لنرفع الآن التغريدات التي نحتاجها في كل مدينة. وسنقلل من حجم المدونة لنتجنب عناء الإشكاليات التي تتعلق بالذاكرة."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             City                                               Text\n",
      "0      washington   you really need to go back to bar tending or ...\n",
      "1          london   jay finley christ in explains why today is co...\n",
      "2           lagos   forget if this happened truly it s definitely...\n",
      "3         toronto   yall i love this skin big thanks to for makin...\n",
      "4         nairobi   the late brilliant prof ali mazrui explains h...\n",
      "...           ...                                                ...\n",
      "64995     toronto   taxing issues federal government to charge gs...\n",
      "64996   bengaluru   this is revealing of americans are held hosta...\n",
      "64997      mumbai   pumps in pumping station did not work in my s...\n",
      "64998     atlanta   why is it that some ppl can speak amp act w i...\n",
      "64999     phoenix   you netflix in already ready for more seasons...\n",
      "\n",
      "[65000 rows x 2 columns]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "file = \"sociolinguistics.english_cities.gz\"\n",
    "file = os.path.join(ai.data_dir, file)\n",
    "df = pd.read_csv(file, nrows=65000)\n",
    "print(df)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "والآن رفعت بياناتنا. لنلق نظرة عامة على المدن مرة أخرى. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "washington 3263\n",
      "london 4154\n",
      "lagos 3057\n",
      "toronto 3268\n",
      "nairobi 2505\n",
      "melbourne 1125\n",
      "perth 946\n",
      "chicago 3614\n",
      "dallas 3013\n",
      "chennai 1230\n",
      "delhi 2506\n",
      "mumbai 2608\n",
      "atlanta 2787\n",
      "bengaluru 2528\n",
      "seattle 1046\n",
      "johannesburg 2772\n",
      "adelaide 968\n",
      "miami 1990\n",
      "calgary 1177\n",
      "boston 3672\n",
      "phoenix 1096\n",
      "auckland 899\n",
      "brisbane 1054\n",
      "karachi 1282\n",
      "new_york 3042\n",
      "los_angeles 2500\n",
      "san_francisco 3113\n",
      "singapore 1743\n",
      "kolkata 1273\n",
      "syndney 769\n"
     ]
    }
   ],
   "source": [
    "freqs = ai.print_labels(df, \"City\")\n",
    "for city in freqs:\n",
    "    print(city, freqs[city])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "إلى الآن لدينا (1) بيانات من كل مدينة و(2) محتوى متجهي (عبارات وأوزان TF-IDF من مدونات رقمية ضخمة). وسنصنف هذه العينات بحسب المدينة. وأدناه الكود البرمجي الأساسي، وفيه فقط استدعاء لحزمتنا *text_analytics*. وتلك الحزمة تقسم البيانات إلى بيانات تدريب وبيانات اختبار، ثم تدرب المصنف وتقيمه. وقد وجهنا الحزمة لاستعمال \"المدينة City\" صنفا أساسيا مع خصائص المحتوى.\n",
    "\n",
    "ولننظر الآن في خصائص المفردات.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHRASES:\n",
      "['united_states', 'boris_johnson', 'chuck_schumer', 'ricky_gervais', 'rashida_tlaib', 'hillary_clinton', 'devin_nunes', 'sherlock_holmes', 'edinson_cavani', 'climate_change', 'harry_potter', 'happy_birthday']\n",
      "\n",
      "VOCAB\n",
      "['people', 'new', 'today', 'need', 'want', 'video', 'life', 'world', 'year', 'please', 'watch', 'thanks']\n"
     ]
    }
   ],
   "source": [
    "#ai.fit_phrases(df, min_count=1, language=\"en\")\n",
    "#ai.fit_tfidf(df, n_features = 10000)\n",
    "\n",
    "file_phrases = os.path.join(ai.data_dir, \"sociolinguistics.english_all.gz\")\n",
    "ai.phrases = ai.deserialize(\"phrases\", file_phrases + \".phrases.json\")\n",
    "ai.tfidf_vectorizer = ai.deserialize(\"tfidf_model\", file + \".tfidf.json\")\n",
    "\n",
    "print(\"PHRASES:\")\n",
    "example_phrases = list(ai.phrases.phrasegrams.keys())\n",
    "print(example_phrases[:12])\n",
    "\n",
    "print(\"\\nVOCAB\")\n",
    "example_vocab = list(ai.tfidf_vectorizer.vocabulary_.keys())\n",
    "print(example_vocab[:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "وما هو مختلف هنا هو أننا في المرة الأخيرة استعملنا SVM، وهذه المرة سنستعمل MLP. وSVM تتدرب على كل شيء مرة واحدة، فيما تتدرب MLP مع مرور الزمن؛ ولذلك سنتمكن من تتبع النموذج أثناء تعلمه."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1829/1829 [==============================] - 35s 18ms/step - loss: 0.4933 - accuracy: 0.8494 - val_loss: 0.1701 - val_accuracy: 0.9440\n",
      "Epoch 2/25\n",
      "1829/1829 [==============================] - 32s 17ms/step - loss: 0.0420 - accuracy: 0.9875 - val_loss: 0.2116 - val_accuracy: 0.9374\n",
      "Epoch 3/25\n",
      "1829/1829 [==============================] - 33s 18ms/step - loss: 0.0172 - accuracy: 0.9945 - val_loss: 0.2840 - val_accuracy: 0.9243\n",
      "Epoch 4/25\n",
      "1829/1829 [==============================] - 33s 18ms/step - loss: 0.0170 - accuracy: 0.9945 - val_loss: 0.2946 - val_accuracy: 0.9328\n",
      "Epoch 5/25\n",
      "1829/1829 [==============================] - 25s 14ms/step - loss: 0.0132 - accuracy: 0.9955 - val_loss: 0.3180 - val_accuracy: 0.9292\n",
      "Epoch 6/25\n",
      "1829/1829 [==============================] - 25s 14ms/step - loss: 0.0112 - accuracy: 0.9960 - val_loss: 0.3685 - val_accuracy: 0.9265\n",
      "Epoch 7/25\n",
      "1829/1829 [==============================] - 22s 12ms/step - loss: 0.0098 - accuracy: 0.9967 - val_loss: 0.3816 - val_accuracy: 0.9278\n",
      "Epoch 8/25\n",
      "1829/1829 [==============================] - 22s 12ms/step - loss: 0.0079 - accuracy: 0.9974 - val_loss: 0.4218 - val_accuracy: 0.9235\n",
      "Epoch 9/25\n",
      "1829/1829 [==============================] - 23s 12ms/step - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.4658 - val_accuracy: 0.9248\n",
      "Epoch 10/25\n",
      "1829/1829 [==============================] - 26s 14ms/step - loss: 0.0073 - accuracy: 0.9976 - val_loss: 0.4124 - val_accuracy: 0.9317\n",
      "Epoch 11/25\n",
      "1829/1829 [==============================] - 26s 14ms/step - loss: 0.0063 - accuracy: 0.9981 - val_loss: 0.4461 - val_accuracy: 0.9249\n",
      "Epoch 12/25\n",
      "1829/1829 [==============================] - 24s 13ms/step - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.4823 - val_accuracy: 0.9260\n",
      "Epoch 13/25\n",
      "1829/1829 [==============================] - 22s 12ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.4887 - val_accuracy: 0.9311\n",
      "Epoch 14/25\n",
      "1829/1829 [==============================] - 22s 12ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.5513 - val_accuracy: 0.9306\n",
      "Epoch 15/25\n",
      "1829/1829 [==============================] - 23s 13ms/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.5198 - val_accuracy: 0.9334\n",
      "Epoch 16/25\n",
      "1829/1829 [==============================] - 26s 14ms/step - loss: 0.0048 - accuracy: 0.9984 - val_loss: 0.5646 - val_accuracy: 0.9300\n",
      "Epoch 17/25\n",
      "1829/1829 [==============================] - 26s 14ms/step - loss: 0.0044 - accuracy: 0.9985 - val_loss: 0.6120 - val_accuracy: 0.9269\n",
      "Epoch 18/25\n",
      "1829/1829 [==============================] - 23s 13ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 0.5416 - val_accuracy: 0.9314\n",
      "Epoch 19/25\n",
      "1829/1829 [==============================] - 21s 12ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.5914 - val_accuracy: 0.9303\n",
      "Epoch 20/25\n",
      "1829/1829 [==============================] - 22s 12ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.6486 - val_accuracy: 0.9288\n",
      "Epoch 21/25\n",
      "1829/1829 [==============================] - 25s 14ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.5962 - val_accuracy: 0.9315\n",
      "Epoch 22/25\n",
      "1829/1829 [==============================] - 25s 14ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.6187 - val_accuracy: 0.9297\n",
      "Epoch 23/25\n",
      "1829/1829 [==============================] - 26s 14ms/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 0.6546 - val_accuracy: 0.9249\n",
      "Epoch 24/25\n",
      "1829/1829 [==============================] - 23s 12ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.6531 - val_accuracy: 0.9288\n",
      "Epoch 25/25\n",
      "1829/1829 [==============================] - 21s 12ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.7650 - val_accuracy: 0.9249\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     adelaide       0.85      0.92      0.88        97\n",
      "      atlanta       0.91      0.94      0.92       270\n",
      "     auckland       0.99      0.94      0.96        80\n",
      "    bengaluru       0.93      0.90      0.92       242\n",
      "       boston       0.93      0.90      0.92       366\n",
      "     brisbane       0.93      0.82      0.87       109\n",
      "      calgary       0.97      0.98      0.97       118\n",
      "      chennai       0.96      0.98      0.97       122\n",
      "      chicago       0.96      0.81      0.88       359\n",
      "       dallas       0.79      0.97      0.87       299\n",
      "        delhi       0.94      0.94      0.94       255\n",
      " johannesburg       1.00      1.00      1.00       254\n",
      "      karachi       1.00      1.00      1.00       120\n",
      "      kolkata       0.90      0.98      0.94       124\n",
      "        lagos       1.00      1.00      1.00       288\n",
      "       london       0.98      0.97      0.98       436\n",
      "  los_angeles       0.87      0.81      0.84       249\n",
      "    melbourne       0.84      0.78      0.81       115\n",
      "        miami       0.96      0.88      0.92       213\n",
      "       mumbai       0.92      0.89      0.90       266\n",
      "      nairobi       0.99      1.00      1.00       265\n",
      "     new_york       0.85      0.88      0.86       297\n",
      "        perth       0.83      0.89      0.86        89\n",
      "      phoenix       0.96      0.78      0.86       101\n",
      "san_francisco       0.92      0.95      0.93       315\n",
      "      seattle       0.93      0.79      0.86        86\n",
      "    singapore       0.98      1.00      0.99       180\n",
      "      syndney       0.89      0.73      0.80        81\n",
      "      toronto       0.88      0.98      0.93       350\n",
      "   washington       0.91      0.98      0.94       354\n",
      "\n",
      "     accuracy                           0.92      6500\n",
      "    macro avg       0.93      0.91      0.92      6500\n",
      " weighted avg       0.93      0.92      0.92      6500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = ai.mlp(df, labels = \"City\", features = \"content\", layers = [500, 500])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**طوّل بالك**\n",
    "\n",
    "هذا كل ما في الأمر! ننظر في دقة المصنف.\n",
    "وسيتغير ذلك عما في المبحث قليلا لأننا نستعمل فصلا عشوائيا لبيانات التدريب والاختبار. وهذا يعني أن المصنف ينظر في بيانات مختلفة كل مرة. وإذا ما أردت أمثلة إضافية متقدمة على حل إشكالية تصنيف المدن، انظر في دالة *text_analytics.mlp()*.\n"
   ]
  }
 ],
 "metadata": {
  "direction": "rtl",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
