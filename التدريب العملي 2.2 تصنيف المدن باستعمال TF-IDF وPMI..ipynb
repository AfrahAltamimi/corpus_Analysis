{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "سوف نصنف في هذا النشاط المدن اعتمادا على المحتوى وبالنظر في التغريدات. وهذه نفس دراسة الحالة التي أشرنا إليها في المبحث. وسنبدأ بتحميل متطلباتنا."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from text_analytics import TextAnalytics\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ai = TextAnalytics()\n",
    "ai.data_dir = os.path.join(\"..\", \"data\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "لنحمّل التغريدات التي نحتاجها لكل مدينة."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              City                                               Text\n",
      "0       washington   you really need to go back to bar tending or ...\n",
      "1           london   jay finley christ in explains why today is co...\n",
      "2            lagos   forget if this happened truly it s definitely...\n",
      "3          toronto   yall i love this skin big thanks to for makin...\n",
      "4          nairobi   the late brilliant prof ali mazrui explains h...\n",
      "...            ...                                                ...\n",
      "150025     chennai   finally indian women team make victory in las...\n",
      "150026     chennai   syngene international ltd calls for board mee...\n",
      "150027     chennai   true no one takes you seriously such a senile...\n",
      "150028     chennai   do you really need to test your site to impro...\n",
      "150029     chennai   one becomes father of the nation and another ...\n",
      "\n",
      "[150030 rows x 2 columns]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "file = \"sociolinguistics.english_cities.gz\"\n",
    "file = os.path.join(ai.data_dir, file)\n",
    "df = pd.read_csv(file)\n",
    "print(df)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "والآن طبقنا خصائص المحتوى على البيانات، بأسلوبين: الأول تعرفنا فيه على العبارات باستعمال مقياس المعلومات المتبادلة النقطي PMI، والثاني: تعرفنا فيه على أوزان تكرار الكلمة في النص ومعكوسه في النصوص TF-IDF من خلال حساب عدد الكلمات في النصوص. وكلتا هاتين الخطوتين قد تستغرقا وقتا. ولذلك حملنا النماذج التي نحتاجها مسبقا. لكن قد تشغل الكود أدناه إذا أردت أن تفعل ذلك بنفسك (احذف وسم التعليق أولا). ويمكنك أيضا التحقق من حزمة *text_analytics* للنظر في ذلك بمزيد من التفصيل. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHRASES:\n",
      "['united_states', 'boris_johnson', 'chuck_schumer', 'ricky_gervais', 'rashida_tlaib', 'hillary_clinton', 'devin_nunes', 'sherlock_holmes', 'edinson_cavani', 'climate_change', 'harry_potter', 'happy_birthday']\n",
      "\n",
      "VOCAB\n",
      "['people', 'new', 'today', 'need', 'want', 'video', 'life', 'world', 'year', 'please', 'watch', 'thanks']\n"
     ]
    }
   ],
   "source": [
    "#ai.fit_phrases(df, min_count=1, language=\"en\")\n",
    "#ai.fit_tfidf(df, n_features = 10000)\n",
    "\n",
    "file_phrases = os.path.join(ai.data_dir, \"sociolinguistics.english_all.gz\")\n",
    "ai.phrases = ai.deserialize(\"phrases\", file_phrases + \".phrases.json\")\n",
    "ai.tfidf_vectorizer = ai.deserialize(\"tfidf_model\", file + \".tfidf.json\")\n",
    "\n",
    "print(\"PHRASES:\")\n",
    "example_phrases = list(ai.phrases.phrasegrams.keys())\n",
    "print(example_phrases[:12])\n",
    "\n",
    "print(\"\\nVOCAB\")\n",
    "example_vocab = list(ai.tfidf_vectorizer.vocabulary_.keys())\n",
    "print(example_vocab[:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "حتى الآن لدينا (1) بياناتنا من تويتر، و(2) متجه كامل محتوانا (عبارات TF-IDF + PMI). وسنصنفها بحسب المدينة. والكود البرمجي الأساسي أدناه. وفيه فقط استدعاء لحزمتنا *text_analytics*. وتلك الحزمة تقسم البيانات إلى بيانات تدريب وبيانات اختبار، ثم تدرب المصنف وتقيمه. ونوجه الحزمة لاستعمال \"المدينة City\" صنفا أساسيا مع خصائص المحتوى."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9726)\t0.0749937420567999\n",
      "  (0, 9658)\t0.07497708911780718\n",
      "  (0, 9090)\t0.07364115740517338\n",
      "  (0, 8973)\t0.0742041893027956\n",
      "  (0, 8645)\t0.07289270227746461\n",
      "  (0, 8327)\t0.07222838937187062\n",
      "  (0, 8291)\t0.0724010225801125\n",
      "  (0, 8092)\t0.0721626588101253\n",
      "  (0, 8043)\t0.0721103367495788\n",
      "  (0, 7998)\t0.07186492724429036\n",
      "  (0, 7980)\t0.07187771706160526\n",
      "  (0, 7853)\t0.07153716713149491\n",
      "  (0, 7660)\t0.07067434669250264\n",
      "  (0, 7623)\t0.07080215164329921\n",
      "  (0, 7439)\t0.07051366281361417\n",
      "  (0, 7078)\t0.06972018033040071\n",
      "  (0, 6990)\t0.06974151596970049\n",
      "  (0, 6941)\t0.1399358018127286\n",
      "  (0, 6928)\t0.06908731221807107\n",
      "  (0, 6552)\t0.06875858350367162\n",
      "  (0, 6512)\t0.06856372184379768\n",
      "  (0, 6508)\t0.06797932641569877\n",
      "  (0, 6464)\t0.06798852610842035\n",
      "  (0, 6230)\t0.06718801231956584\n",
      "  (0, 6206)\t0.0675637123540012\n",
      "  :\t:\n",
      "  (150029, 48)\t0.023706245439530375\n",
      "  (150029, 47)\t0.02252345595296407\n",
      "  (150029, 46)\t0.04476143061578029\n",
      "  (150029, 45)\t0.02245941325645736\n",
      "  (150029, 40)\t0.06812059389201963\n",
      "  (150029, 37)\t0.021483270101145046\n",
      "  (150029, 36)\t0.02128846174813955\n",
      "  (150029, 35)\t0.021202662232335398\n",
      "  (150029, 34)\t0.02081829627797108\n",
      "  (150029, 33)\t0.22836261702694766\n",
      "  (150029, 31)\t0.02086143531666654\n",
      "  (150029, 29)\t0.020442650215937292\n",
      "  (150029, 26)\t0.02030370089851719\n",
      "  (150029, 25)\t0.04048081536353046\n",
      "  (150029, 24)\t0.041699784426901323\n",
      "  (150029, 23)\t0.040981630386342296\n",
      "  (150029, 19)\t0.020692785393040935\n",
      "  (150029, 15)\t0.019329261718506924\n",
      "  (150029, 14)\t0.019277403240324793\n",
      "  (150029, 11)\t0.018861756979774123\n",
      "  (150029, 8)\t0.017862306877731562\n",
      "  (150029, 6)\t0.04882831655320408\n",
      "  (150029, 2)\t0.014299063369558909\n",
      "  (150029, 1)\t0.06569472690056806\n",
      "  (150029, 0)\t0.012849400221812517\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "x, vocab_size = ai.get_features(df, features = \"content\")\n",
    "print(x)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     adelaide       0.91      0.92      0.91       529\n",
      "      atlanta       0.93      0.93      0.93       481\n",
      "     auckland       0.98      0.99      0.99       498\n",
      "    bengaluru       0.94      0.96      0.95       503\n",
      "       boston       0.94      0.93      0.93       441\n",
      "     brisbane       0.91      0.88      0.90       484\n",
      "      calgary       1.00      0.97      0.98       527\n",
      "      chennai       0.99      0.98      0.99       513\n",
      "      chicago       0.92      0.92      0.92       545\n",
      "       dallas       0.95      0.94      0.94       496\n",
      "        delhi       0.93      0.97      0.95       499\n",
      " johannesburg       1.00      1.00      1.00       503\n",
      "      karachi       1.00      1.00      1.00       523\n",
      "      kolkata       0.97      0.96      0.97       512\n",
      "        lagos       1.00      1.00      1.00       462\n",
      "       london       0.98      0.97      0.97       486\n",
      "  los_angeles       0.86      0.95      0.90       498\n",
      "    melbourne       0.86      0.85      0.85       496\n",
      "        miami       0.97      0.93      0.95       497\n",
      "       mumbai       0.95      0.91      0.93       494\n",
      "      nairobi       1.00      1.00      1.00       501\n",
      "     new_york       0.91      0.93      0.92       515\n",
      "        perth       0.92      0.90      0.91       456\n",
      "      phoenix       0.96      0.93      0.94       527\n",
      "san_francisco       0.96      0.95      0.95       499\n",
      "      seattle       0.95      0.94      0.95       525\n",
      "    singapore       0.99      1.00      1.00       486\n",
      "      syndney       0.89      0.90      0.89       493\n",
      "      toronto       0.96      0.97      0.96       519\n",
      "   washington       0.95      0.97      0.96       495\n",
      "\n",
      "     accuracy                           0.95     15003\n",
      "    macro avg       0.95      0.95      0.95     15003\n",
      " weighted avg       0.95      0.95      0.95     15003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = ai.shallow_classification(df, labels = \"City\", features = \"content\", classifier = \"lm\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**طوّل بالك**\n",
    "\n",
    "هذا كل ما في الأمر! ننظر في دقة المصنف.\n",
    "\n",
    "وسيتغير ذلك عما في المحاضرة قليلا لأننا نستعمل فصلا عشوائيا لبيانات التدريب والاختبار. وهذا يعني أن المصنف ينظر في بيانات مختلفة كل مرة. وإذا ما أردت أمثلة إضافية متقدمة على حل إشكالية تصنيف المدن، انظر في دالة *text_analytics.shallow_classification.\n"
   ]
  }
 ],
 "metadata": {
  "direction": "rtl",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
