{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "لنعمل الآن مع تضمين الكلمات. لقد كنا نركز غالبا على النصوص، لكننا الآن سننظر في الكلمات نظرة مباشرة باستعمال ووردتوفيك word2vec. وسنرى كيف تكون هذه التضمينات، ثم سنستعملها لاستكشاف تشابه الكلمات.\n",
    "\n",
    "\n",
    "فلنبدأ بتجهيز بياناتنا.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from text_analytics import TextAnalytics\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ai = TextAnalytics()\n",
    "ai.data_dir = os.path.join(\"..\", \"data\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "والآن، يستغرق تعلم تضمين الكلمات وقتا طويلا جدا أطول مما تستغرقه المهام التي كنا نجريها. ولذلك سنرفع تضمينات كلمات مدربة مسبقا. فإذا *كنت* تريد أن تجري ذلك بنفسك فالكود البرمجي هنا مضافا إليه علامة التعليق #. ويمكن أن تجد تفاصيل أكثر في حزمة * text_analytics*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1235968   0.04748945  0.16285081 ...  0.03241363 -0.03300684\n",
      "   0.26601192]\n",
      " [-0.0030584   0.04572405  0.10358463 ... -0.00173236 -0.10827369\n",
      "   0.20938973]\n",
      " [-0.19914334  0.03242941  0.0766376  ...  0.12763536  0.02289162\n",
      "   0.0425519 ]\n",
      " ...\n",
      " [ 0.05791736  0.24080583  0.11852352 ...  0.13066971  0.03170295\n",
      "   0.20842037]\n",
      " [-0.05558461  0.00972249  0.02807915 ...  0.00696762  0.19038115\n",
      "   0.08795328]\n",
      " [ 0.04808212  0.17134945 -0.06976075 ...  0.02019391  0.03982212\n",
      "   0.0915439 ]]\n",
      "['the_DET', 'of_ADP', 'a_DET', 'and_CCONJ', 'in_ADP', 'number_NOUN', 'to_PART', 'to_ADP', 'for_ADP', 'on_ADP', 'at_ADP', 'is_AUX', 'by_ADP', 'was_AUX', 'with_ADP', 'that_SCONJ', 'as_SCONJ', 'his_DET', 'from_ADP', 'it_PRON']\n"
     ]
    }
   ],
   "source": [
    "file = \"economic.nyt.1931-2016.gz\"\n",
    "file = os.path.join(ai.data_dir, file)\n",
    "#df = pd.read_csv(file)\n",
    "#print(df)\n",
    "\n",
    "#ai.train_word2vec(df)\n",
    "\n",
    "ai.word_vectors = ai.deserialize(\"w2v_embedding\", file + \".w2v_embedding.json\")\n",
    "ai.word_vectors_vocab = ai.deserialize(\"w2v_vocab\", file + \".w2v_vocab.json\")\n",
    "    \n",
    "print(ai.word_vectors)\n",
    "print(list(ai.word_vectors_vocab.keys())[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "لقد رفعنا الآن تضمينات الكلمات. وقد دربت تلك التضمينات على المقالات الافتتاحية في *صحيفة نيويورك تايمز* من عام 1931 إلى عام 2016. ولنجهزهم للعمل الآن."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build an index of each word\n",
    "y = list(ai.word_vectors_vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "لنلق نظرة على بعض كلماتنا."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shovels_VERB', 'resplendent_VERB', 'manguel_PROPN', 'chappell_PROPN', 'northwoods_NOUN', 'warble_ADJ', 'ryberg_PROPN', 'advances_PROPN', 'paradorn_PROPN', 'courtin_PROPN']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sample = random.sample(ai.word_vectors_vocab.keys(), 10)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "من المؤكد أنك ستلاحظ فورا أن هذه ليست كلمات عادية وحسب! وإنما أسند إليها وسوم من نحو: اسم، وفعل. فما الذي حدث؟\n",
    "أولا، استعملنا PMI للوقوف على العبارات *قبل* تدريبنا لتضمينات الكلمات. فمثلا نظرنا إلى \"downton abbey\" في العينة العشوائية التي نبحث عنها. وهي كيان واحد، واستعمالنا PMI هنا يوضح كيف أن دلالة الكلمتين مجتمعة تختلف عن دلالتهما منفصلتين.\n",
    "ثانيا، نرى وسوما، مثل: اسم، وفعل ... . وتلك تعرف بوسوم أقسام الكلام أو POS اختصارا. وتشير إلى القسم النحوي الذي تنتمي إليه كل كلمة. وقد اعتمدنا في ذلك على حزمة * spaCy*. ويمكن أن ترى آلية وكيفية ذلك بالنظر في الدالتين *train_word2vec* و* clean* في حزمتنا *text_analytics*. وسيتيح لنا ذلك التصنيفات أدناه التي قد تكون معروفة بالنسبة لك من قبل. وإن لم يكن كذلك، استعن بهذا الرابط لقراءة المزيد عنها [here](https://universaldependencies.org/u/pos/all.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Open-class words  |  Closed-class words  |  Other  |\n",
    "| :------------- | :----------:            | ------: |\n",
    "|ADJ (adjective)         | ADP (preposition) \t                   | PUNCT (punctuation)  |\n",
    "|ADV (adverb)\t         | AUX (auxiliary verb)\t                   | SYM (symbol)    |\n",
    "|INTJ (interjection)         | CCONJ (coordinating conjunction)                   | X  (misc)     |\n",
    "|NOUN (noun) \t         | DET (determiner, like \"the\")\t                   |         |\n",
    "|PROPN (proper noun)      | NUM (number)\t                   |         |\n",
    "|VERB (verb)\t         | PART  (participle)                  |         |\n",
    "                 | PRON (pronoun)                   |         |\n",
    "\t             | SCONJ (subordinating conjunction)                   |\t     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "هنا توضيح بسيط، وهو أن *open-class words* عبارة عن تصنيفات مفتوحة غير محدودة الكلمات في كل لغة، حيث يمكننا مثلا توليد أسماء جديدة دائما. ولذلك تلك الوسوم هي ما سنعلم إسنادها للكلمات التي لم تُر من قبل. أما *closed-class words* فهي تصنيفات محدودة وثابتة. وكل كلمات هذا النوع في الإنجليزية مثلا عبارة عن كلمات وظيفية. إذن لدينا بعض المعلومات المتعلقة بكلمات هذا النوع، وسنكون دائما على معرفة بها."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "إذن لماذا أضفنا هذه الوسوم النحوية قبل أن ندرب على تضمين الكلمات؟ تلك هي أسهل الأساليب لإزالة الغموض عن الكلمات. ولقد تكلمنا في هذا المبحث عن كلمات مثل \"table\" التي قد تكون اسما كما قد تكون فعلا. وذلك النوع من الكلمات قد يكون في تغير فسمه الكلامي بمعان مختلفة تماما وأحيانا غير متصلّة أبدا. ولذلك نضيف القسم الكلامي للكلمات حتى نتتبع معنى الكلمة الذي نهدف للتعامل معه.\n",
    "\n",
    "والآن لنبدأ العمل مع الأكواد البرمجية! سيقف الكود البرمجي أدناه على كلمة واحدة وقوفا عشوائيا، ثم يظهرها لنا، وبعد ذلك يكشف عن الكلمات الخمس الأكثر شبها بها. ويحسب *التشابه* هنا باستعمال مقياس تشابه جيب التمام بالتطبيق على الكلمات بدلا من النصوص.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embarrassingly_ADV ['shockingly_ADV', 'curiously_ADV', 'utterly_ADV', 'unimaginative_ADJ', 'hopelessly_ADV']\n"
     ]
    }
   ],
   "source": [
    "sample = random.sample(ai.word_vectors_vocab.keys(), 1)[0]\n",
    "index = ai.word_vectors_vocab[sample]\n",
    "sample, closest = ai.linguistic_distance(x = ai.word_vectors, y = y, sample = index, n = 5, metric=\"cosine\")\n",
    "\n",
    "print(sample, closest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "هذا كل شيء! في كل مرة تعيد فيها تشغيل الكود البرمجي سترى مجموعة مختلفة من الكلمات المتشابهة.\n",
    "\n",
    "ولنرفع تضمين الكلمات الآن من مدونة خطابات الكونقرس التي تنطوي على نفس الفترة الزمنية.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = \"economic.congress.1931-2016.gz\"\n",
    "file2 = os.path.join(ai.data_dir, file2)\n",
    "\n",
    "congress_word_vectors = ai.deserialize(\"w2v_embedding\", file2 + \".w2v_embedding.json\")\n",
    "congress_word_vectors_vocab = ai.deserialize(\"w2v_vocab\", file2 + \".w2v_vocab.json\")\n",
    "\n",
    "#Build an index of each word\n",
    "congress_y = list(congress_word_vectors_vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "والآن اختر أي كلمة من قاعدة بيانات واحدة، فإذا وجدت في كلا القاعدتين، احسب الكلمات الأكثر شبها بها من المدونات \n",
    "لنقف الآن على كلمة واحدة من قاعدة بيانات واحدة، وإذا وجدت في كلا القاعدتين، نبحث عن الكلمات الأكثر شبها بها في المدونتين.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "furs_VERB\n",
      "\n",
      " NYT furs_VERB ['damasks_NOUN', 'blouses_VERB', 'washandwear_NOUN', 'redingotes_NOUN', 'crushable_ADJ', 'polkadot_ADJ', 'pucci_ADJ', 'negligees_NOUN', 'tunics_PROPN', 'charmeuse_NOUN']\n",
      "\n",
      " Congress furs_VERB ['lipsticks_NOUN', 'toys_VERB', 'silverware_VERB', 'bicycles_VERB', 'dyes_VERB', 'medicinals_NOUN', 'lotions_NOUN', 'shampoos_NOUN', 'handbags_VERB', 'motorcycles_VERB']\n"
     ]
    }
   ],
   "source": [
    "sample = random.sample(ai.word_vectors_vocab.keys(), 1)[0]\n",
    "index = ai.word_vectors_vocab[sample]\n",
    "\n",
    "if sample in congress_word_vectors_vocab:\n",
    "    print(sample)\n",
    "    congress_index = congress_word_vectors_vocab[sample]\n",
    "    \n",
    "    sample, closest = ai.linguistic_distance(x = ai.word_vectors, y = y, sample = index, n = 10, metric=\"cosine\")\n",
    "    print(\"\\n\", \"NYT\", sample, closest)\n",
    "    \n",
    "    sample, closest = ai.linguistic_distance(x = congress_word_vectors, y = congress_y, sample = congress_index, n = 10, metric=\"cosine\")\n",
    "    print(\"\\n\", \"Congress\", sample, closest)\n",
    "    \n",
    "else:\n",
    "    print(\"Try again! This word is not found in both corpora.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "direction": "rtl",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
